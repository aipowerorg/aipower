"use strict";(self.webpackChunkhelp=self.webpackChunkhelp||[]).push([[3799],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var m=a.createContext({}),p=function(e){var t=a.useContext(m),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=p(e.components);return a.createElement(m.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,m=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),u=p(n),g=r,d=u["".concat(m,".").concat(g)]||u[g]||c[g]||i;return n?a.createElement(d,o(o({ref:t},s),{},{components:n})):a.createElement(d,o({ref:t},s))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=g;var l={};for(var m in t)hasOwnProperty.call(t,m)&&(l[m]=t[m]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},5327:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>l,default:()=>g,frontMatter:()=>o,metadata:()=>m,toc:()=>s});var a=n(7462),r=(n(7294),n(3905));const i=n.p+"assets/images/max-tokens-f473a4105de392d6f61470cc8cd005fa.png",o={sidebar_position:6},l="Maximum Length",m={unversionedId:"ai-engine/openai/max-tokens",id:"ai-engine/openai/max-tokens",title:"Maximum Length",description:"What is Maximum Length?",source:"@site/docs/ai-engine/openai/max-tokens.mdx",sourceDirName:"ai-engine/openai",slug:"/ai-engine/openai/max-tokens",permalink:"/docs/ai-engine/openai/max-tokens",draft:!1,editUrl:"https://github.com/aipowerorg/aipowerorg.github.io/edit/main/docs/ai-engine/openai/max-tokens.mdx",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Temperature",permalink:"/docs/ai-engine/openai/temperature"},next:{title:"Frequency Penalty",permalink:"/docs/ai-engine/openai/frequency-penalty"}},p={},s=[{value:"What is Maximum Length?",id:"what-is-maximum-length",level:2},{value:"Comparing Token Limits",id:"comparing-token-limits",level:2},{value:"Adjusting the Maximum Length Setting",id:"adjusting-the-maximum-length-setting",level:2},{value:"Impact of Max Tokens Value on Generated Content",id:"impact-of-max-tokens-value-on-generated-content",level:2}],u={toc:s},c="wrapper";function g(e){let{components:t,...n}=e;return(0,r.kt)(c,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"maximum-length"},"Maximum Length"),(0,r.kt)("h2",{id:"what-is-maximum-length"},"What is Maximum Length?"),(0,r.kt)("p",null,"Language models process text by dividing it into tokens, which can be words or groups of characters. "),(0,r.kt)("p",null,'For example, "fantastic" splits into "fan", "tas", and "tic", while "gold" is a single token. Many tokens start with a space, like " hi" and " greetings".'),(0,r.kt)("p",null,"The number of tokens depends on the input and output text length. "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"1 token \u2248 4 characters in English"),(0,r.kt)("li",{parentName:"ul"},"1 token \u2248 \xbe words"),(0,r.kt)("li",{parentName:"ul"},"100 tokens \u2248 75 words"),(0,r.kt)("li",{parentName:"ul"},"1-2 sentences \u2248 30 tokens"),(0,r.kt)("li",{parentName:"ul"},"1 paragraph \u2248 100 tokens"),(0,r.kt)("li",{parentName:"ul"},"1,500 words \u2248 2,048 tokens")),(0,r.kt)("p",null,"The combined length of the text prompt and generated completion must not exceed the model's maximum context length, usually 4096 tokens (about 3000 words). Prices are based on 1,000 tokens, which is about 750 words."),(0,r.kt)("h2",{id:"comparing-token-limits"},"Comparing Token Limits"),(0,r.kt)("p",null,"Understanding token limits helps in selecting the right model for your tasks. Here\u2019s a comparison of the maximum tokens for various models:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Model Group"),(0,r.kt)("th",{parentName:"tr",align:null},"Model"),(0,r.kt)("th",{parentName:"tr",align:null},"Maximum Length"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"GPT-4 Models"),(0,r.kt)("td",{parentName:"tr",align:null},"gpt-4"),(0,r.kt)("td",{parentName:"tr",align:null},"8,192")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"gpt-4o"),(0,r.kt)("td",{parentName:"tr",align:null},"4,096")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"gpt-4-32k"),(0,r.kt)("td",{parentName:"tr",align:null},"32,768")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"GPT-3.5 Models"),(0,r.kt)("td",{parentName:"tr",align:null},"gpt-3.5-turbo"),(0,r.kt)("td",{parentName:"tr",align:null},"4,096")))),(0,r.kt)("h2",{id:"adjusting-the-maximum-length-setting"},"Adjusting the Maximum Length Setting"),(0,r.kt)("p",null,"The default Maximum Length is ",(0,r.kt)("strong",{parentName:"p"},"1500"),". You can change it in the ",(0,r.kt)("strong",{parentName:"p"},"AI Settings")," tab."),(0,r.kt)("p",null,"Steps to change the Maximum Length:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Go to the plugin menu on your WordPress dashboard."),(0,r.kt)("li",{parentName:"ol"},"Click on the ",(0,r.kt)("strong",{parentName:"li"},"Dashboard")," page and find the ",(0,r.kt)("strong",{parentName:"li"},"AI Settings")," tab."),(0,r.kt)("li",{parentName:"ol"},"Enter a new value in the ",(0,r.kt)("strong",{parentName:"li"},"Maximum Length")," field.")),(0,r.kt)("img",{src:i}),(0,r.kt)("p",null,"Experiment with different values to find the best setting."),(0,r.kt)("h2",{id:"impact-of-max-tokens-value-on-generated-content"},"Impact of Max Tokens Value on Generated Content"),(0,r.kt)("p",null,"Max Tokens controls the maximum number of tokens generated in a single call to the GPT model. It limits the output size."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Useful for generating specific amounts of text."),(0,r.kt)("li",{parentName:"ul"},"Helps fit generated text into specific formats or applications."),(0,r.kt)("li",{parentName:"ul"},"Can improve model performance.")),(0,r.kt)("p",null,"However, setting Max Tokens too low can lead to incomplete sentences or grammatically incorrect text."),(0,r.kt)("p",null,"Max Tokens are important in scenarios like:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Generating a specific amount of text."),(0,r.kt)("li",{parentName:"ul"},"Constraining text to fit a format."),(0,r.kt)("li",{parentName:"ul"},"Improving performance.")),(0,r.kt)("p",null,"Be careful not to set it too low to avoid incomplete thoughts and grammatical errors."))}g.isMDXComponent=!0}}]);